<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sari Sandbox is a high-fidelity, photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks.">
  <meta name="keywords" content="Sari, Sari Sandbox, SariBench">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  
<!-- TODO: Add home page to  -->
<!-- Commented out until more research can be added -->
<!-- These are examples from Nerfies -->
<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Janika Deborah Gajo</a>,<sup>1</sup>
            </span>
            <span class="author-block">
              <a href="#">Gerarld Paul Merales</a>,<sup>1</sup>
            </span>
            <span class="author-block">
              <a href="#">Jerome Escarcha</a>,<sup>1</sup>
            </span>
            <span class="author-block">
              <a href="#">Brenden Ashley Molina</a>,<sup>1</sup>
            </span>
            <span class="author-block">
              <a href="#">Gian Nartea</a>,<sup>1</sup>
            </span>
            <span class="author-block">
              <a href="#">Emmanuel Maminta</a>,<sup>2</sup>
            </span>
            <span class="author-block">
              <a href="#">Juan Carlos Roldan</a>,<sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://roatienza.github.io/">Rowel Atienza</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>EEEI, University of the Philippines, Diliman, Quezon City</span>
            <span class="author-block"><sup>2</sup>AI Graduate Program, University of the Philippines, Diliman, Quezon City</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- TODO: Fill out the arxiv/somewhere link -->
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <!-- TODO: Fill out the arxiv/somewhere link -->
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- TODO: Add YouTube video -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- TODO: Add GitHub link -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- If we had one, it would go here. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="#"
                type="video/mp4">
      </video> -->
      <img src="./static/images/sari-banner-min.jpg" alt="Sari-sari Sandbox banner">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Sari Sandbox</span>: A high-fidelity virtual retail environment for benchmarking embodied AI agents and humans. Features include randomized product placement, multiple store layouts, interactive elements, VR support, and a Python API for task control. Both human participants and agents perform comparable shopping tasks for evaluation.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/human-demo-3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/human-demo-4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/human-demo-5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/human-demo-6.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/human-demo-7.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/human-demo-8.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/human-demo-9.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <span class="dnerf">Sari Sandbox</span>, a high-fidelity, photorealistic 3D retail store simulation for
            benchmarking embodied agents against human performance in shopping tasks. Addressing a gap in retail-specific sim
            environments for embodied agent training, <span class="dnerf">Sari Sandbox</span> features over 250 interactive grocery items across three
            store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a
            vision language model (VLM)-powered embodied agent. We introduce <span class="dnerf">SariBench</span>, a dataset
            of annotated human demonstrations across varied task difficulties. Our sandbox enables embodied agents to navigate,
            inspect, and manipulate retail items, providing baselines against human performance. We conclude with benchmarks,
            performance analysis, and recommendations for enhancing realism and scalability. <em>The source code and dataset
            will be open-sourced upon publication</em>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <video autoplay muted loop playsinline>
            <source src="./static/videos/human-demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Motivation</h2>
          <p>
            Real-world retail environments are complex, variable, and costly to replicate at scale. Sari Sandbox was built to fill this gap—providing a high-fidelity, controllable testbed for embodied agents in retail shopping tasks. It enables controlled experimentation on perception, navigation, and manipulation under realistic conditions, with support for both human participants and AI agents.
          </p>
          <!-- <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video> -->
        </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Environment Design</h2>
    <div class="content has-text-justified">
      <p>
        The <strong>Sari Sandbox</strong> environment features three store layouts with randomized item placement. It includes over 250 interactive high-fidelity grocery products across shelves, freezers, and checkout counters. This design enables training and evaluation of agents in diverse, high-fidelity conditions.
      </p>
    </div>
    <div class="columns is-multiline is-centered">
      <div class="column is-half">
        <figure class="image">
          <img src="./static/images/figure_5_page5_0.png" alt="Environment layout 1">
        </figure>
      </div>
      <div class="column is-half">
        <figure class="image">
          <img src="./static/images/figure_5_page5_1.png" alt="Environment layout 2">
        </figure>
      </div>
    </div>
    <div class="columns is-multiline is-centered">
      <div class="column is-half">
        <figure class="image">
          <img src="./static/images/highFidelity1.png" alt="SariBench task flowchart 1">
        </figure>
      </div>
      <div class="column is-half">
        <figure class="image">
          <img src="./static/images/highFidelity2.png" alt="SariBench task flowchart 2">
        </figure>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">SariBench</h2>
    <div class="content has-text-justified">
      <p>
        <strong>SariBench</strong> is a dataset and benchmark suite designed to evaluate both human and agent performance on shopping-related tasks. It spans 3 core task types (availability, location, pricing) and includes intermediate and hard tasks requiring manipulation, reasoning, and product comparison.
      </p>
    </div>
    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Difficulty</th>
            <th>Skills Involved</th>
            <th>Example Task</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Easy</td>
            <td>Perception, Navigation, Manipulation</td>
            <td>Find and pick up a box of cereal.</td>
          </tr>
          <tr>
            <td>Average</td>
            <td>Perception, Navigation, Manipulation, Memory, Task Execution</td>
            <td>Pick up a bottle of soda and scan at checkout.</td>
          </tr>
          <tr>
            <td>Difficult</td>
            <td>Perception, Navigation, Manipulation, Memory, Task Execution, Decision Making, Comprehension</td>
            <td>Which of these two products has lower sugar content: strawberry-flavored biscuit or chocolate-flavored biscuit? Scan the answer.</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">VR Playground</h2>
    <div class="content has-text-justified">
      <p>
        Human participants interact with the environment using VR controllers to grab, inspect, and scan products. This enables fine-grained benchmarking of human behavior in embodied tasks.
      </p>
    </div>
    <div class="columns is-multiline is-centered">
      <div class="column is-two-thirds">
        <figure class="image">
          <img src="./static/images/figure_9_page6_0.png" alt="VR Participants">
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Agent Architecture</h2>
    <div class="content has-text-justified">
      <p>
        Our embodied agent is composed of three main modules: a Vision Language Model (VLM)-based captioner, a planner that converts high-level goals into structured steps, and a decider that maps these into low-level actions. This architecture supports semantic navigation, object interaction, and episodic memory.
      </p>
    </div>
    <div class="columns is-centered">
      <div class="column is-two-thirds">
        <figure class="image">
          <img src="./static/images/EmbodiedAgentRichGraphic.png" alt="Agent loop architecture">
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Human vs. Agent Performance</h2>
    <div class="content has-text-justified">
      <p>
        We compare human performance to an embodied agent baseline across varying task difficulty levels. Metrics include:
        <strong>HAT</strong> (Human Average Time), <strong>HCR%</strong> (Human Completion Rate), <strong>AAT</strong> (Agent Average Time), and <strong>ACR%</strong> (Agent Completion Rate).
      </p>
    </div>

    <div class="table-container">
      <table class="table is-bordered is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Difficulty</th>
            <th>HAT ↓</th>
            <th>HCR% ↑</th>
            <th>AAT ↓</th>
            <th>ACR% ↑</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>Easy-L1</td><td>47</td><td>88.88</td><td>780</td><td>68.63</td></tr>
          <tr><td>Easy-L2</td><td>73</td><td>100.00</td><td>660</td><td>45.10</td></tr>
          <tr><td>Easy-L3</td><td>61</td><td>93.33</td><td>420</td><td>33.33</td></tr>
          <tr><td>Average-L1</td><td>158</td><td>87.50</td><td>-</td><td>-</td></tr>
          <tr><td>Average-L2</td><td>106</td><td>100.00</td><td>-</td><td>-</td></tr>
          <tr><td>Average-L3</td><td>84</td><td>100.00</td><td>-</td><td>-</td></tr>
          <tr><td>Difficult-L1</td><td>76</td><td>100.00</td><td>-</td><td>-</td></tr>
          <tr><td>Difficult-L2</td><td>136</td><td>100.00</td><td>-</td><td>-</td></tr>
          <tr><td>Difficult-L3</td><td>113</td><td>100.00</td><td>-</td><td>-</td></tr>
        </tbody>
      </table>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gajo2025sari
  author    = {Gajo, Janika Deborah and Merales, Gerarld Paul and Escarcha, Jerome and Molina, Brenden Ashley and Nartea, Gian and Maminta, Emmanuel and Roldan, Juan Carlos and Atienza, Rowel},
  title     = {Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents},
  journal   = {ICCV 2025 Workshop RetailVision},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- TODO: Link to PDF -->
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/sarisandbox/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on source code from a website licensed under a 
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Original source: <a href="https://nerfies.github.io/">nerfies.github.io</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
